#jinja2:lstrip_blocks: True

# PKI defines the location of credentials for this node. Each of these can also be inlined by using the yaml ": |" syntax.
pki:
  # The CAs that are accepted by this node. Must contain one or more certificates created by 'nebula-cert ca'
  ca: /var/lib/nebula/ca.crt
  cert: /var/lib/nebula/{{ inventory_hostname }}.crt
  key: /var/lib/nebula/{{ inventory_hostname }}.key
  #blacklist is a list of certificate fingerprints that we will refuse to talk to
  #blacklist:
  #  - c99d4e650533b92061b09918e838a5a0a6aaee21eed1d12fd937682865936c72

# The static host map defines a set of hosts with fixed IP addresses on the internet (or any network).
# A host can have multiple fixed IP addresses defined here, and nebula will try each when establishing a tunnel.
# The syntax is:
#   "{nebula ip}": ["{routable ip/dns name}:{routable port}"]
# Example, if your lighthouse has the nebula IP of 192.168.100.1 and has the real ip address of 100.64.22.11 and runs on port 4242:
static_host_map:
{% for host in lighthouses %}
  {% if hostvars[host].nebula_ip is defined and hostvars[host].nebula_port is defined and hostvars[host].external_addr is defined %}
  "{{ hostvars[host].nebula_ip }}": ["{{ hostvars[host].external_addr }}:{{ hostvars[host].nebula_port }}"]
  {% endif %}
{% endfor %}


lighthouse:
  am_lighthouse: {{ nebula_lighthouse }}
  serve_dns: {{ nebula_dns }}
  {% if nebula_dns == 'true' %}
  dns:
    host: {{ nebula_dns_host }}
    port: {{ nebula_dns_port }}
  {% endif %}
  interval: {{ nebula_update_interval }}
  # hosts is a list of lighthouse hosts this node should report to and query from
  # IMPORTANT: THIS SHOULD BE EMPTY ON LIGHTHOUSE NODES
  hosts:
  {% if not 'lighthouse' in group_names %}
    {% for host in lighthouses %}
    {% if hostvars[host].nebula_ip is defined %}
    - "{{ hostvars[host].nebula_ip }}"
    {% endif %}
    {% endfor %}
  {% endif %}

listen:
  host: {{ nebula_host }}
  port: {{ nebula_port }}
  # Sets the max number of packets to pull from the kernel for each syscall (under systems that support recvmmsg)
  # default is 64, does not support reload
  #batch: 64
  # Configure socket buffers for the udp side (outside), leave unset to use the system defaults. Values will be doubled by the kernel
  # Default is net.core.rmem_default and net.core.wmem_default (/proc/sys/net/core/rmem_default and /proc/sys/net/core/rmem_default)
  # Maximum is limited by memory in the system, SO_RCVBUFFORCE and SO_SNDBUFFORCE is used to avoid having to raise the system wide
  # max, net.core.rmem_max and net.core.wmem_max
  #read_buffer: 10485760
  #write_buffer: 10485760

# Punchy continues to punch inbound/outbound at a regular interval to avoid expiration of firewall nat mappings
punchy: true
# punch_back means that a node you are trying to reach will connect back out to you if your hole punching fails
# this is extremely useful if one node is behind a difficult nat, such as symmetric
#punch_back: true

# Cipher allows you to choose between the available ciphers for your network.
# IMPORTANT: this value must be identical on ALL NODES/LIGHTHOUSES. We do not/will not support use of different ciphers simultaneously!
#cipher: chachapoly

local_range: {{ local_range }}

# sshd can expose informational and administrative functions via ssh this is a
#sshd:
  # Toggles the feature
  #enabled: true
  # Host and port to listen on, port 22 is not allowed for your safety
  #listen: 127.0.0.1:2222
  # A file containing the ssh host private key to use
  # A decent way to generate one: ssh-keygen -t ed25519 -f ssh_host_ed25519_key -N "" < /dev/null
  #host_key: ./ssh_host_ed25519_key
  # A file containing a list of authorized public keys
  #authorized_users:
    #- user: steeeeve
      # keys can be an array of strings or single string
      #keys:
        #- "ssh public key string"

# Configure the private interface. Note: addr is baked into the nebula certificate
tun:
  dev: {{ nebula_dev_tun }}
  drop_local_broadcast: {{ nebula_drop_local_broadcast }}
  drop_multicast: {{ nebula_drop_multicast }}
  tx_queue: {{ nebula_tx_queue }}
  mtu: {{ nebula_mtu }}
  {% if nebula_routes is defined %}
  routes:
  {% for route in nebula_routes %}
  - mtu: {{route.mtu}}
    route: {{route.network}}
  {% endfor %}
  {% endif %}
  {% if nebula_unsafe_routes is defined %}
  unsafe_routes:
  {% for unsafe_route in nebula_unsafe_routes %}
  - route: {{ unsafe_route.network }}
    via: {{ unsafe_route.gateway }}
    {% if unsafe_route.mtu is defined %}
    mtu: {{ unsafe_route.mtu }}
    {% endif %}
  {% endfor %}
  {% endif %}

logging:
  level: info
  format: text

  type: prometheus
  listen: {{ nebula_prometheus_host }}:{{ nebula_prometheus_port }}
  path: /{{ nebula_prometheus_path }}
  namespace: prometheus
  subsystem: nebula
  interval: 10s

# Nebula security group configuration
firewall:
  conntrack:
    tcp_timeout: 120h
    udp_timeout: 3m
    default_timeout: 10m
    max_connections: 100000

  # The firewall is default deny. There is no way to write a deny rule.
  # Rules are comprised of a protocol, port, and one or more of host, group, or CIDR
  # Logical evaluation is roughly: port AND proto AND (ca_sha OR ca_name) AND (host OR group OR groups OR cidr)
  # - port: Takes `0` or `any` as any, a single number `80`, a range `200-901`, or `fragment` to match second and further fragments of fragmented packets (since there is no port available).
  #   code: same as port but makes more sense when talking about ICMP, TODO: this is not currently implemented in a way that works, use `any`
  #   proto: `any`, `tcp`, `udp`, or `icmp`
  #   host: `any` or a literal hostname, ie `test-host`
  #   group: `any` or a literal group name, ie `default-group`
  #   groups: Same as group but accepts a list of values. Multiple values are AND'd together and a certificate would have to contain all groups to pass
  #   cidr: a CIDR, `0.0.0.0/0` is any.
  #   ca_name: An issuing CA name
  #   ca_sha: An issuing CA shasum

  outbound:
    {{ nebula_outbound_rules | indent( width=4 ) }}

  inbound:
    {{ nebula_inbound_rules | indent( width=4 ) }}
